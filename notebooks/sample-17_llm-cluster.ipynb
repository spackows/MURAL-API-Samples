{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a large language model to cluster sticky notes\n",
    "\n",
    "### Scenario\n",
    "Imagine you are a team lead and you are taking your team through a reflection activity.  Your team has used a mural to record their feedback.  Now, you need to cluster the sticky notes in the mural for further analysis.\n",
    "\n",
    "Notebook sections:\n",
    "\n",
    "**Section A - Explore prompting large languge models**\n",
    "- [Step 1: Set up IBM watsonx.ai foundation model Python library prerequisites](#step1)\n",
    "- [Step 2: Create a function for prompting a model to identify top themes in messages](#step2)\n",
    "- [Step 3: Create a function for prompting a model to classify a message](#step3)\n",
    "\n",
    "**Section B - Set up your mural**\n",
    "- [Step 4: Set up MURAL prerequisites](#step4)\n",
    "- [Step 5: Read feedback messages from your mural](#step5)\n",
    "\n",
    "**Section C - Cluster sticky notes in the mural**\n",
    "- [Step 6: Identify top themes in sticky notes](#step6)\n",
    "- [Step 7: Classify sticky notes](#step7)\n",
    "- [Step 8: Move sticky notes into labelled boxes in the mural](#step8)\n",
    "\n",
    "When you run this notebook, your mural will look something like the following image:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/spackows/MURAL-API-Samples/main/images/sample-17_llm-cluster_01.png\" width=\"50%\" title=\"Image of a mural\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section A - Explore prompting large languge models\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/spackows/MURAL-API-Samples/main/images/sample-17_llm-cluster_02.png\" width=\"60%\" title=\"Image of a mural\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step1\"></a>\n",
    "## Step 1: Set up IBM watsonx.ai foundation model Python library prerequisites\n",
    "Before you can prompt a foundation model in watsonx.ai, you must perform the following setup tasks:\n",
    "- 1.1 Create an instance of the Watson Machine Learning service\n",
    "- 1.2 Associate the Watson Machine Learning instance with the current project\n",
    "- 1.3 Create an IBM Cloud API key\n",
    "- 1.4 Create a credentials dictionary for Watson Machine learning\n",
    "- 1.5 Look up the current project ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create an instance of the Watson Machine Learning service\n",
    "If you don't already have an instance of the IBM Watson Machine Learning service, you can create an instance of the service from the IBM Cloud catalog: <a href=\"https://console.ng.bluemix.net/catalog/services/ibm-watson-machine-learning/\" target=\"_blank\">Watson Machine Learning service</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Associate an instance of the Watson Machine Learning service with the current project\n",
    "The _current project_ is the project in which you are running this notebook.\n",
    "\n",
    "If an instance of Watson Machine Learning is not already associated with the current project, follow the instructions in this topic to do so: <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/assoc-services.html?context=wx&audience=wdp\" target=\"_blank\">Adding associated services to a project</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create an IBM Cloud API key\n",
    "Create an IBM Cloud API key by following these instruction: <a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui#create_user_key\" target=\"_blank\">Creating an IBM Cloud API key</a>\n",
    "\n",
    "Then paste your new IBM Cloud API key in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_cloud_apikey = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Create a credentials dictionary for Watson Machine learning\n",
    "See: [Authentication](https://ibm.github.io/watson-machine-learning-sdk/setup_cloud.html#authentication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://ibm.github.io/watson-machine-learning-sdk/setup_cloud.html#authentication\n",
    "g_wml_credentials = { \n",
    "    \"url\"    : \"https://us-south.ml.cloud.ibm.com\", \n",
    "    \"apikey\" : g_cloud_apikey\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Look up the current project ID\n",
    "The _current project_ is the project in which you are running this notebook.  You can get the ID of the current project programmatically by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "g_project_id = os.environ[\"PROJECT_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step2\"></a>\n",
    "## Step 2: Create a function for prompting a model to identify top themes in sticky notes\n",
    "- 2.1 Experiment in Prompt Lab\n",
    "- 2.2 Specify your selected model ID, prompt parameters, and prompt text template\n",
    "- 2.2 Define a function to identify themes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Experiment in Prompt Lab \n",
    "The Prompt Lab in watsonx.ai is a graphical interface for experimenting with prompting foundation models.\n",
    "\n",
    "Experiment in Prompt Lab to discover what works best:\n",
    "- Which model returns ideal results\n",
    "- What parameter settings (eg. decoding) work best\n",
    "- What prompt text causes the model to respond the way you want\n",
    "\n",
    "See: [Prompt Lab](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-prompt-lab.html?context=wx&audience=wdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Specify your selected model_id, prompt parameters, and prompt text template\n",
    "In the following three cells, there are example model ID, prompt parameters, and a prompt text template you can use.\n",
    "\n",
    "Replace any of these with values you discovered while experimenting in Prompt Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example models:\n",
    "#\n",
    "# google/flan-ul2\n",
    "# google/flan-t5-xxl\n",
    "# bigscience/mt0-xxl\n",
    "# eleutherai/gpt-neox-20b\n",
    "# ibm/granite-13b-instruct-v1\n",
    "\n",
    "g_themes_model_id = \"meta-llama/llama-2-70b-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_themes_prompt_parameters = {\n",
    "    \"decoding_method\" : \"greedy\",\n",
    "    \"min_new_tokens\"  : 0,\n",
    "    \"max_new_tokens\"  : 20,\n",
    "    \"stop_sequences\"  : [ \"\\n\\n\" ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "In the following example template, notice the use of `%s` as a placeholder for the messages in the sticky notes. \n",
    "\n",
    "If you replace this template with a prompt you discovered through your experiments in Prompt Lab, remember to include a placeholder for the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_themes_prompt_template = \"\"\"The following meals fall into three broad themes:\n",
    "- Spaghetti with meatballs\n",
    "- Vegetarian sub\n",
    "- Shrimp pad thai\n",
    "- Fish fingers\n",
    "- Falafel\n",
    "- Steak and kidney pie\n",
    "\n",
    "Three themes in the list of meals:\n",
    "- Meat\n",
    "- Vegetarian\n",
    "- Seafood\n",
    "\n",
    "\n",
    "The following animals fall into three broad themes:\n",
    "- Cow\n",
    "- Chicken\n",
    "- Dog\n",
    "- Giraffe\n",
    "- Gerbil\n",
    "- Elephant\n",
    "\n",
    "Three themes in the list of animals:\n",
    "- Pet\n",
    "- Farm\n",
    "- Wild\n",
    "\n",
    "\n",
    "The following process feedback ideas fall into three broad themes:\n",
    "%s\n",
    "\n",
    "Think past what each idea mentions and consider larger patterns across the ideas.\n",
    "\n",
    "Three themes that get at the essence of the list of process feedback ideas:\n",
    "\"\"\"\n",
    "\n",
    "def createThemesPromptText( messages_arr ):\n",
    "    messages_str = \"- \" + \"\\n- \".join( messages_arr )\n",
    "    return g_themes_prompt_template % ( messages_str )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_test_messages_arr = [ \n",
    "    \"Our daily stand-ups take too long.  Let's stop doing those.\",\n",
    "    \"Can we please stop having to sing the national anthem at the start of every day?!\", \n",
    "    \"Stop having all-hands meetings, because they don't add any value.\", \n",
    "    \"Stop forcing everyone to attend the all-hands in person\", \n",
    "    \"I like the \\\"updates\\\" Slack channel - very efficient\",\n",
    "    \"Loving the on-site day care\",\n",
    "    \"Please let's continue the THINK-Thursday meeting-free afternoon!\",\n",
    "    \"The monthly performance awards are a nice recognition\",\n",
    "    \"I think we should record our meetings so they could be played back\",\n",
    "    \"At my friend's company, they have a lottery for plum parking spots.  We should do something similar!\",\n",
    "    \"We should take meeting minutes\",\n",
    "    \"Because of these power outages, we should invest in uninterrupted power supply infrastructure\"\n",
    "]\n",
    "g_themes_test_prompt_text = createThemesPromptText( g_test_messages_arr )\n",
    "print( g_themes_test_prompt_text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define a function to identify themes\n",
    "You can prompt foundation models in IBM watsonx.ai programmatically using the foundation models Python library.\n",
    "\n",
    "See:\n",
    "- <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-python-lib.html?context=wx&audience=wdp\" target=\"_blank\">Introduction to the foundation models Python library</a>\n",
    "- <a href=\"https://ibm.github.io/watson-machine-learning-sdk/foundation_models.html\" target=\"_blank\">Foundation models Python library reference</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "themes_test_response:\n",
      "{\n",
      "   \"model_id\": \"meta-llama/llama-2-70b-chat\",\n",
      "   \"created_at\": \"2023-11-15T00:00:58.117Z\",\n",
      "   \"results\": [\n",
      "      {\n",
      "         \"generated_text\": \"- Efficiency\\n- Culture\\n- Communication\\n\\n\",\n",
      "         \"generated_token_count\": 13,\n",
      "         \"input_token_count\": 389,\n",
      "         \"stop_reason\": \"stop_sequence\"\n",
      "      }\n",
      "   ],\n",
      "   \"system\": {\n",
      "      \"warnings\": [\n",
      "         {\n",
      "            \"message\": \"This model is a Non-IBM Product governed by a third-party license that may impose use restrictions and other obligations. By using this model you agree to its terms as identified in the following URL. URL: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx\",\n",
      "            \"id\": \"disclaimer_warning\"\n",
      "         }\n",
      "      ]\n",
      "   }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "import json\n",
    "\n",
    "themes_test_model = Model( g_themes_model_id, g_wml_credentials, g_themes_prompt_parameters, g_project_id )\n",
    "\n",
    "themes_test_response = themes_test_model.generate( g_themes_test_prompt_text )\n",
    "\n",
    "print( \"themes_test_response:\\n\" + json.dumps( themes_test_response, indent=3 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate( model_id, prompt_parameters, prompt_text, b_debug=False ):\n",
    "    model = Model( model_id, g_wml_credentials, prompt_parameters, g_project_id )\n",
    "    raw_response = model.generate( prompt_text )\n",
    "    if b_debug:\n",
    "        print( \"\\nraw_response:\\n\" + json.dumps( raw_response, indent=3 ) )\n",
    "    if ( \"results\" in raw_response ) \\\n",
    "       and ( len( raw_response[\"results\"] ) > 0 ) \\\n",
    "       and ( \"generated_text\" in raw_response[\"results\"][0] ):\n",
    "        return raw_response, raw_response[\"results\"][0][\"generated_text\"]\n",
    "    else:\n",
    "        print( \"\\nThe model failed to generate an answer\" )\n",
    "        print( \"\\nDebug info:\\n\" + json.dumps( raw_response, indent=3 ) )\n",
    "        return raw_response, \"\"\n",
    "\n",
    "def identifyThemes( model_id, prompt_parameters, messages_arr, b_debug=False ):\n",
    "    prompt_text = createThemesPromptText( messages_arr )\n",
    "    if b_debug:\n",
    "        print( \"Prompt:\\n--------------------------START\")\n",
    "        print( prompt_text + \"--------------------------END\")\n",
    "    raw_response, generated_output = generate( model_id, prompt_parameters, prompt_text, b_debug )\n",
    "    class_names_str = re.sub( r\"^\\s*\\-\\s*\", \"\", generated_output )\n",
    "    class_names_str = re.sub( r\"\\s+$\", \"\", class_names_str )\n",
    "    class_names_arr = re.split( r\"\\s*\\n+\\-\\s*\", class_names_str )\n",
    "    class_names_arr = [ item.strip().capitalize() for item in class_names_arr ]\n",
    "    return class_names_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_test_themes_arr = identifyThemes( g_themes_model_id, g_themes_prompt_parameters, g_test_messages_arr, b_debug=True )\n",
    "\n",
    "print( \"\\ng_test_themes_arr:\\n\" + json.dumps( g_test_themes_arr, indent=3 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step3\"></a>\n",
    "## Step 3: Create a function for prompting a model to classify sticky notes\n",
    "- 3.1 Experiment in Prompt Lab again\n",
    "- 3.2 Specify your selected model ID, prompt parameters, and prompt text template\n",
    "- 3.2 Define a function to perform classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Experiment in Prompt Lab again\n",
    "Experiment in Prompt Lab to discover what works best:\n",
    "- Which model returns ideal results\n",
    "- What parameter settings (eg. decoding) work best\n",
    "- What prompt text causes the model to respond the way you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Specify your selected model_id, prompt parameters, and prompt text template\n",
    "In the following three cells, there are example model ID, prompt parameters, and a prompt text template you can use.\n",
    "\n",
    "Replace any of these with values you discovered while experimenting in Prompt Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example models:\n",
    "#\n",
    "# google/flan-ul2\n",
    "# google/flan-t5-xxl\n",
    "# bigscience/mt0-xxl\n",
    "# eleutherai/gpt-neox-20b\n",
    "# ibm/granite-13b-instruct-v1\n",
    "\n",
    "g_classify_model_id = \"google/flan-t5-xxl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_classify_prompt_parameters = {\n",
    "    \"decoding_method\" : \"greedy\",\n",
    "    \"min_new_tokens\"  : 0,\n",
    "    \"max_new_tokens\"  : 20\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "In the following example template, notice the use of `%s` as a placeholder for the class names and message text. \n",
    "\n",
    "If you replace this template with a prompt you discovered through your experiments in Prompt Lab, remember to include a placeholder for the class names and one for the message text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_classify_prompt_template = \"\"\"Classify the message into one of three classes: %s\n",
    "\n",
    "If the message doesn't fit any of the classes, say \"Other\"\n",
    "\n",
    "Message: %s\n",
    "Class: \n",
    "\"\"\"\n",
    "\n",
    "def createClassifyPromptText( class_names_arr, message_text ):\n",
    "    class_name_str = \", \".join( class_names_arr )\n",
    "    return g_classify_prompt_template % ( class_name_str, message_text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the message into one of three classes: Efficiency, Culture, Communication\n",
      "\n",
      "If the message doesn't fit any of the classes, say \"Other\"\n",
      "\n",
      "Message: I think we should record our meetings so they could be played back\n",
      "Class: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "g_classify_test_message = \"I think we should record our meetings so they could be played back\"\n",
    "g_classify_test_prompt_text = createClassifyPromptText( g_test_themes_arr, g_classify_test_message )\n",
    "print( g_classify_test_prompt_text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Define a function to perform classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyMessage( model_id, prompt_parameters, class_names_arr, message_text, b_debug=False ):\n",
    "    prompt_text = createClassifyPromptText( class_names_arr, message_text )\n",
    "    if b_debug:\n",
    "        print( \"Prompt:\\n--------------------------START\")\n",
    "        print( prompt_text + \"--------------------------END\")\n",
    "    raw_response, generated_output = generate( model_id, prompt_parameters, prompt_text, b_debug )\n",
    "    class_name = generated_output.strip().capitalize()\n",
    "    return class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifyMessage( g_classify_model_id, g_classify_prompt_parameters, g_test_themes_arr, g_classify_test_message, b_debug=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section B - Set up your mural\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/spackows/MURAL-API-Samples/main/images/sample-17_llm-cluster_03.png\" width=\"50%\" title=\"Image of a mural\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step4\"></a>\n",
    "## Step 4: Set up MURAL prerequisites\n",
    "- 4.1 Create empty, sample mural\n",
    "- 4.2 Collect mural ID\n",
    "- 4.3 Get Oauth token\n",
    "- 4.4 Populate mural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create empty, sample mural\n",
    "In the MURAL web interface, create a new, empty mural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Collect mural ID\n",
    "You can find the mural ID in the url of a mural.\n",
    "\n",
    "Mural urls look something like this:\n",
    "\n",
    "```\n",
    "https://app.mural.co/t/<workspace>/m/<workspace>/<id>/...\n",
    "```\n",
    "\n",
    "What you need to pass to the MURAL API is just after the `/m/`: the \\<workspace> and the \\<id>.  And you need to join then with a period.\n",
    "\n",
    "For example, if you have a mural with this url:\n",
    "\n",
    "```\n",
    "https://app.mural.co/t/teamideas1234/m/teamideas1234/1234567890123/...\n",
    "```\n",
    "\n",
    "Then, the mural ID is: `teamideas1234.1234567890123`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_mural_id = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Collect OAuth token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_auth_token = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Populate mural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/spackows/MURAL-API-Samples/main/murals/sample-17_llm-cluster.json\"\n",
    "response = urllib.request.urlopen( url )\n",
    "encoding = response.info().get_content_charset( \"utf8\" )\n",
    "sample_widgets_arr = json.loads( response.read().decode( encoding ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import copy\n",
    "\n",
    "def putWidget( auth_token, mural_id, widget ):\n",
    "    # https://developers.mural.co/public/reference/createstickynote\n",
    "    # https://developers.mural.co/public/reference/createtextbox\n",
    "    widget_type = \"textbox\" if ( \"text\" == widget[\"type\"] ) else widget[\"type\"]\n",
    "    url = \"https://app.mural.co/api/public/v1/murals/\" + mural_id + \"/widgets/\" + re.sub( \"\\s+\", \"-\", widget_type )\n",
    "    headers = { \"Accept\"        : \"application/json\", \n",
    "                \"Content-Type\"  : \"application/json\", \n",
    "                \"Authorization\" : \"Bearer \" + auth_token }\n",
    "    parms = copy.deepcopy( widget )\n",
    "    if \"id\" in parms:\n",
    "        del parms[\"id\"]\n",
    "    if \"type\" in parms:\n",
    "        del parms[\"type\"]\n",
    "    response = requests.request( \"POST\", url, headers = headers, json = parms )\n",
    "    response_json = json.loads( response.text )\n",
    "    msg = \"\"\n",
    "    if \"code\" in response_json:\n",
    "        msg += response_json[\"code\"] + \" \"\n",
    "    if \"message\" in response_json:\n",
    "        msg += response_json[\"message\"]\n",
    "    if msg != \"\":\n",
    "        print( \"[ \" + widget[\"id\"] + \" ] \" + msg )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Quick!  After running this cell, switch to your browser \n",
    "# tab where the mural is to see the change\n",
    "# ...\n",
    "\n",
    "for widget in sample_widgets_arr:\n",
    "    putWidget( g_auth_token, g_mural_id, widget )\n",
    "print( \"Done!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step5\"></a>\n",
    "## Step 5: Read feedback from your mural\n",
    "- 5.1 Read widgets from the mural\n",
    "- 5.2 Establish coordinates of class boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Read widgets from the mural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listWidgets( auth_token, mural_id ):\n",
    "    # https://developers.mural.co/public/reference/getmuralwidgets\n",
    "    url = \"https://app.mural.co/api/public/v1/murals/\" + mural_id + \"/widgets\"\n",
    "    headers = { \"Accept\": \"application/json\", \"Authorization\": \"Bearer \" + auth_token }\n",
    "    response = requests.request( \"GET\", url, headers = headers )\n",
    "    response_json = json.loads( response.text )\n",
    "    msg = \"\"\n",
    "    if \"code\" in response_json:\n",
    "        msg += response_json[\"code\"] + \" \"\n",
    "    if \"message\" in response_json:\n",
    "        msg += response_json[\"message\"]\n",
    "    if msg != \"\":\n",
    "        print( msg )\n",
    "        return None\n",
    "    if \"value\" not in response_json:\n",
    "        print( \"No value returned\" )\n",
    "        return None\n",
    "    return response_json[\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_widgets_arr = listWidgets( g_auth_token, g_mural_id )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( json.dumps( g_widgets_arr, indent=3 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Establish coordinates of boxes\n",
    "In the mural, there are three boxes, one box for each theme the LLM generates.\n",
    "\n",
    "To be able to move the sticky notes into the correct box by theme, we need to know the coordinates of the boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWidgetsByType( widgets_arr ):\n",
    "    boxes_arr = []\n",
    "    stickies_arr = []\n",
    "    for widget in widgets_arr:\n",
    "        obj = { \"x\" : widget[\"x\"],\n",
    "                \"y\" : widget[\"y\"],\n",
    "                \"height\" : widget[\"height\"],\n",
    "                \"width\"  : widget[\"width\"] }\n",
    "        if( \"shape\" == widget[\"type\"] ):\n",
    "            boxes_arr.append( obj )\n",
    "        elif( \"sticky note\" == widget[\"type\"] ):\n",
    "            obj[\"id\"]     = widget[\"id\"]\n",
    "            obj[\"text\"]   = widget[\"text\"]\n",
    "            obj[\"height\"] = widget[\"height\"]\n",
    "            obj[\"width\"]  = widget[\"width\"]\n",
    "            obj[\"x_org\"]  = widget[\"x\"]\n",
    "            obj[\"y_org\"]  = widget[\"y\"]\n",
    "            stickies_arr.append( obj )\n",
    "    return boxes_arr, stickies_arr\n",
    "\n",
    "def getMuralObjects( widgets_arr ):\n",
    "    boxes_arr, stickies_arr = getWidgetsByType( widgets_arr )\n",
    "    print( \"Boxes:\" )\n",
    "    for i in range( len( boxes_arr ) ):\n",
    "        print( \"Box \" + str( i+1 ) + \": ( x=\" + str( boxes_arr[i][\"x\"] ) + \", y=\" + str( boxes_arr[i][\"y\"] ) + \" )\" )\n",
    "    print( \"\\nStickies:\")\n",
    "    for sticky in stickies_arr:\n",
    "        print( sticky[\"text\"] )\n",
    "    return boxes_arr, stickies_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boxes:\n",
      "Box 1: ( x=75, y=-450 )\n",
      "Box 2: ( x=600, y=-450 )\n",
      "Box 3: ( x=1100, y=-450 )\n",
      "\n",
      "Stickies:\n",
      "Our daily stand-ups take too long. Let's stop doing those.\n",
      "Can we please stop having to sing the national anthem at the start of every day?!\n",
      "Stop having all-hands meetings, because they don't add any value.\n",
      "Stop forcing everyone to attend the all-hands in person\n",
      "I like the \"updates\" Slack channel - very efficient\n",
      "Loving the on-site day care\n",
      "Please let's continue the THINK-Thursday meeting-free afternoon!\n",
      "The monthly performance awards are a nice recognition\n",
      "I think we should record our meetings so they could be played back\n",
      "At my friend's company, they have a lottery for plum parking spots. We should do something similar!\n",
      "We should take meeting minutes\n",
      "Because of these power outages, we should invest in uninterrupted power supply infrastructure\n"
     ]
    }
   ],
   "source": [
    "g_boxes_arr, g_stickies_arr = getMuralObjects( g_widgets_arr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section C - Cluster sticky notes in the mural\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/spackows/MURAL-API-Samples/main/images/sample-17_llm-cluster_01.png\" width=\"50%\" title=\"Image of a mural\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step6\"></a>\n",
    "## Step 6: Identify top themes in sticky notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifyThemesInStickies( model_id, prompt_parameters, stickies_arr, b_debug=False ):\n",
    "    messages_arr = []\n",
    "    for sticky in stickies_arr:\n",
    "        messages_arr.append( sticky[\"text\"] )\n",
    "    class_names_arr = identifyThemes( model_id, prompt_parameters, messages_arr, b_debug )\n",
    "    return class_names_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "--------------------------START\n",
      "The following meals fall into three broad themes:\n",
      "- Spaghetti with meatballs\n",
      "- Vegetarian sub\n",
      "- Shrimp pad thai\n",
      "- Fish fingers\n",
      "- Falafel\n",
      "- Steak and kidney pie\n",
      "\n",
      "Three themes in the list of meals:\n",
      "- Meat\n",
      "- Vegetarian\n",
      "- Seafood\n",
      "\n",
      "\n",
      "The following animals fall into three broad themes:\n",
      "- Cow\n",
      "- Chicken\n",
      "- Dog\n",
      "- Giraffe\n",
      "- Gerbil\n",
      "- Elephant\n",
      "\n",
      "Three themes in the list of animals:\n",
      "- Pet\n",
      "- Farm\n",
      "- Wild\n",
      "\n",
      "\n",
      "The following process feedback ideas fall into three broad themes:\n",
      "- Our daily stand-ups take too long. Let's stop doing those.\n",
      "- Can we please stop having to sing the national anthem at the start of every day?!\n",
      "- Stop having all-hands meetings, because they don't add any value.\n",
      "- Stop forcing everyone to attend the all-hands in person\n",
      "- I like the \"updates\" Slack channel - very efficient\n",
      "- Loving the on-site day care\n",
      "- Please let's continue the THINK-Thursday meeting-free afternoon!\n",
      "- The monthly performance awards are a nice recognition\n",
      "- I think we should record our meetings so they could be played back\n",
      "- At my friend's company, they have a lottery for plum parking spots. We should do something similar!\n",
      "- We should take meeting minutes\n",
      "- Because of these power outages, we should invest in uninterrupted power supply infrastructure\n",
      "\n",
      "Think past what each idea mentions and consider larger patterns across the ideas.\n",
      "\n",
      "Three themes that get at the essence of the list of process feedback ideas:\n",
      "--------------------------END\n",
      "\n",
      "raw_response:\n",
      "{\n",
      "   \"model_id\": \"meta-llama/llama-2-70b-chat\",\n",
      "   \"created_at\": \"2023-11-15T00:03:11.686Z\",\n",
      "   \"results\": [\n",
      "      {\n",
      "         \"generated_text\": \"- Efficiency\\n- Culture\\n- Communication\\n\\n\",\n",
      "         \"generated_token_count\": 13,\n",
      "         \"input_token_count\": 387,\n",
      "         \"stop_reason\": \"stop_sequence\"\n",
      "      }\n",
      "   ],\n",
      "   \"system\": {\n",
      "      \"warnings\": [\n",
      "         {\n",
      "            \"message\": \"This model is a Non-IBM Product governed by a third-party license that may impose use restrictions and other obligations. By using this model you agree to its terms as identified in the following URL. URL: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx\",\n",
      "            \"id\": \"disclaimer_warning\"\n",
      "         }\n",
      "      ]\n",
      "   }\n",
      "}\n",
      "[\n",
      "   \"Efficiency\",\n",
      "   \"Culture\",\n",
      "   \"Communication\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "g_class_names_arr = identifyThemesInStickies( g_themes_model_id, g_themes_prompt_parameters, g_stickies_arr, b_debug=True )\n",
    "\n",
    "print( json.dumps( g_class_names_arr, indent=3 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step7\"></a>\n",
    "## Step 7: Classify sticky notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyStickies( class_names_arr, stickies_arr ):\n",
    "    for sticky in stickies_arr:\n",
    "        class_name = classifyMessage( g_classify_model_id, g_classify_prompt_parameters, class_names_arr, sticky[\"text\"] )\n",
    "        sticky[\"class_name\"] = class_name\n",
    "    sorted_stickies_arr = sorted( stickies_arr, key=lambda d: d[\"class_name\"] )\n",
    "    print( \"Results:\\n\" )\n",
    "    for sticky in sorted_stickies_arr:\n",
    "        class_name = \"{:<10}\".format( sticky[\"class_name\"] )\n",
    "        print( class_name + \": \" + sticky[\"text\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "\n",
      "Communication: Stop forcing everyone to attend the all-hands in person\n",
      "Communication: I like the \"updates\" Slack channel - very efficient\n",
      "Communication: We should take meeting minutes\n",
      "Culture   : Can we please stop having to sing the national anthem at the start of every day?!\n",
      "Culture   : Loving the on-site day care\n",
      "Culture   : Please let's continue the THINK-Thursday meeting-free afternoon!\n",
      "Culture   : The monthly performance awards are a nice recognition\n",
      "Culture   : At my friend's company, they have a lottery for plum parking spots. We should do something similar!\n",
      "Efficiency: Our daily stand-ups take too long. Let's stop doing those.\n",
      "Efficiency: Stop having all-hands meetings, because they don't add any value.\n",
      "Efficiency: I think we should record our meetings so they could be played back\n",
      "Efficiency: Because of these power outages, we should invest in uninterrupted power supply infrastructure\n"
     ]
    }
   ],
   "source": [
    "classifyStickies( g_class_names_arr, g_stickies_arr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step8\"></a>\n",
    "## Step 8: Move sticky notes into labelled boxes in the mural\n",
    "- 8.1 Label boxes with class names\n",
    "- 8.2 Move stickies into boxes by class name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Label boxes with class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelPosition( box ):\n",
    "    x = box[\"x\"] + 20\n",
    "    y = box[\"y\"] + 20\n",
    "    return x, y\n",
    "\n",
    "def labelWidget( x, y, class_name ):\n",
    "    widget = { \"height\": 75,\n",
    "               \"id\": \"4\",\n",
    "               \"width\": 400,\n",
    "               \"x\": x,\n",
    "               \"y\": y,\n",
    "               \"style\": {\n",
    "                  \"backgroundColor\": \"#FFFFFF00\",\n",
    "                  \"font\": \"proxima-nova\",\n",
    "                  \"fontSize\": 40,\n",
    "                  \"textAlign\": \"left\"\n",
    "               },\n",
    "               \"text\": class_name,\n",
    "               \"type\": \"text\" }\n",
    "    return widget\n",
    "    \n",
    "def addBoxLabel( auth_token, mural_id, box, class_name ):\n",
    "    x, y = labelPosition( box )\n",
    "    widget = labelWidget( x, y, class_name )\n",
    "    putWidget( auth_token, mural_id, widget )\n",
    "    box[\"top_free_space\"] = widget[\"y\"] + widget[\"height\"] + 20\n",
    "    \n",
    "def labelBoxes( auth_token, mural_id, boxes_arr, class_names_arr ):\n",
    "    for i in range( len( boxes_arr ) ):\n",
    "        box = boxes_arr[i]\n",
    "        class_name = class_names_arr[i]\n",
    "        box[\"class_name\"] = class_name\n",
    "        addBoxLabel( auth_token, mural_id, box, class_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Quick!  After running this cell, switch to your browser \n",
    "# tab where the mural is to see the change\n",
    "# ...\n",
    "\n",
    "labelBoxes( g_auth_token, g_mural_id, g_boxes_arr, g_class_names_arr )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Move stickies into boxes by class name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def moveSticky( auth_token, mural_id, sticky_id, new_x, new_y ):\n",
    "    # https://developers.mural.co/public/reference/updatestickynote\n",
    "    url = \"https://app.mural.co/api/public/v1/murals/\" + mural_id + \"/widgets/sticky-note/\" + sticky_id\n",
    "    headers = { \"Accept\"        : \"application/json\", \n",
    "                \"Content-Type\"  : \"application/json\", \n",
    "                \"Authorization\" : \"Bearer \" + auth_token }\n",
    "    parms = { \"x\" : new_x, \"y\" : new_y }\n",
    "    response = requests.request( \"PATCH\", url, headers = headers, json = parms )\n",
    "    response_json = json.loads( response.text )\n",
    "    msg = \"\"\n",
    "    if \"code\" in response_json:\n",
    "        msg += response_json[\"code\"] + \" \"\n",
    "    if \"message\" in response_json:\n",
    "        msg += response_json[\"message\"]\n",
    "    if msg != \"\":\n",
    "        print( \"[ \" + sticky_id + \" ] \" + msg )\n",
    "\n",
    "def moveStickyToBox( auth_token, mural_id, sticky, box ):\n",
    "    y = box[\"top_free_space\"]\n",
    "    min_x = box[\"x\"]\n",
    "    max_x = box[\"x\"] + box[\"width\"] - ( 0.8 * sticky[\"width\"] )\n",
    "    x = random.uniform( min_x, max_x )\n",
    "    moveSticky( auth_token, mural_id, sticky[\"id\"], x, y )\n",
    "    return x, y\n",
    "\n",
    "def boxesJSON( boxes_arr ):\n",
    "    boxes_json = {}\n",
    "    for box in boxes_arr:\n",
    "        class_name = box[\"class_name\"]\n",
    "        boxes_json[ class_name ] = box\n",
    "    return boxes_json\n",
    "\n",
    "def moveStickiesToClassBoxes( auth_token, mural_id, boxes_arr, stickies_arr ):\n",
    "    boxes_json = boxesJSON( boxes_arr )\n",
    "    for sticky in stickies_arr:\n",
    "        class_name = sticky[\"class_name\"]\n",
    "        box = boxes_json[ class_name ]\n",
    "        new_x, new_y = moveStickyToBox( auth_token, mural_id, sticky, box )\n",
    "        box[\"top_free_space\"] = new_y + sticky[\"height\"] + 10\n",
    "    print( \"Done!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "time.sleep(5)\n",
    "\n",
    "# Quick!  After running this cell, switch to your browser \n",
    "# tab where the mural is to see the change\n",
    "# ...\n",
    "\n",
    "moveStickiesToClassBoxes( g_auth_token, g_mural_id, g_boxes_arr, g_stickies_arr )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something like this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/spackows/MURAL-API-Samples/main/images/sample-17_llm-cluster_04.gif\" width=\"50%\" title=\"Image of a mural\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
